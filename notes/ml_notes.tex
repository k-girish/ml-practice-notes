\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb,bbm}
\usepackage{tikz}
\usepackage{multirow, tabularx}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\E}{\mathop{\mathbb{E}}}

\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\newpara}{\leavevmode\newline}
\newcommand{\hrfullline}{\noindent\makebox[\linewidth]{\rule{\paperwidth}{2pt}}}
\newcommand{\mcaly}{\mathcal{Y}}
\newcommand{\xhat}{\hat{x}}

\title{Machine Learning Notes}
\author{Girish Kumar }
\date{June 2020}

\begin{document}
\maketitle

\setcounter{secnumdepth}{0}
\tableofcontents

\section{Linear Regression}

\begin{itemize}
    \item We don't minimize the distance of the points from the plane but rather the residual error (unless they are the same). The residual error is the vertical distance between the plane and the point (rather than the perpendicular distance).
    
    \item The analytical solution of the Linear Regression equation $Xw=y$ is given by, $w_{LS}=(X^TX)^{-1}X^Ty$. The solution only exists when the matrix $X^TX$ is invertible.
    
    \item The $n \times (d+1)$ dimensional matrix $X$ (where $n>d$) is invertible if $X$ is full rank, i.e. $rank(X)=d+1$. So, $X$ should have $d+1$ linearly independent coulumns.
\end{itemize}

\subsection{Probabilistic view of Linear Regression}

Consider a Gaussian probability model with given mean $\mu$ and covariance matrix $\Sigma = \sigma^2I$. Then, the probability of an output $y$ is given as,

\begin{equation*}
    p(y\vert \mu, \sigma^2) = \frac{1}{(2\pi\sigma^2)^{n/2}}exp\left(-\frac{1}{2\sigma^2}(y-\mu)^T(y-\mu)\right) 
\end{equation*}

Let us then parameterize the mean in a variable $w$ by making a key assumption that for the given dataset $X$,
\begin{equation*}
    \mu = Xw
\end{equation*}

By the maximum likelihood principle, the likelihood is maximum when the parameter becomes, 
\begin{align*}
    w_{ML} &= \argmax_w \ln{p(y\vert \mu, \sigma^2)}\\
        &= \argmax_w \ln{p(y\vert Xw, \sigma^2)}\\
        & = \argmax_w -\Vert y-Xw \Vert^2\\
        & = \argmin_w \Vert y-Xw \Vert^2
\end{align*}

Hence one can see that the maximum likelihood solution for this Gaussian model is the same as that of the least square problem for the given data set $X$ and output $y$.\\
Since we have assumed that the output variable $y$ follows the Gaussian probability model, we have also assumed that the error in true vs predicted output will follow the Gaussian probability model, i.e. for each $i = 0,1,2,\dots n$,
\begin{align*}
    y_i &= x_i^Tw+\epsilon_i, \epsilon_i \stackrel{iid}{\sim} N(0,\sigma^2)\\
    y_i &\stackrel{ind}{\sim} N(x_i^Tw,\sigma^2)\\
    y &\sim N(Xw,\sigma^2)\\
\end{align*}

\subsubsection{Using Probability view to check the "goodness" of the solution}

\subsubsection{-- Is true solution expected ?}

Note that,
\begin{align*}
    w_{LS} &= w_{ML}\\
    \E[w_{LS}] &= \E[w_{ML}]\\
    &= \E[(X^TX)^{-1}X^Ty]\\
    &= (X^TX)^{-1}X^T\E[y]\\
    &= (X^TX)^{-1}X^TXw\\
    &= w\\
\end{align*}

Hence, if the output y was really generated with the Gaussian probability model, the expected value of the maximum likelihood solution (linear regression solution) is the same as that of the true value of the parameters. Hence, this method is an \red{unbiased estimator of the true solution} in average.

\subsubsection{-- How much is the variation from the expected solution?}
Before we begin,
\begin{align*}
    Var[y] &= \E[(y-\E[y])(y-\E[y])^T]\\
    Var[y] & = \E[yy^T]-\mu\mu^T\\
    \E[yy^T] &= \Sigma + \mu\mu^T\\
\end{align*}
Then,
\begin{align*}
    Var[w_{ML}] &= \E[(w-\E[w_{ML}])(w-\E[w_{ML}])^T]\\
    & = \E[w_{ML}w_{ML}^T]-\E[w_{ML}]\E[w_{ML}]^T\\
    & = \E[w_{ML}w_{ML}^T]-ww^T\\
    & = \E[ ((X^TX)^{-1}X^Ty) ((X^TX)^{-1}X^Ty)^T]-ww^T\\
    & = \E[(X^TX)^{-1}X^Tyy^TX(X^TX)^{-1}]-ww^T\\
    & = (X^TX)^{-1}X^T \E[yy^T] X(X^TX)^{-1} - ww^T\\
    & = (X^TX)^{-1}X^T (\Sigma + \mu\mu^T)  X(X^TX)^{-1} - ww^T\\
    & = (X^TX)^{-1}X^T (\sigma^2I + Xww^TX^T)  X(X^TX)^{-1} - ww^T\\ 
    & = (\sigma^2I)(X^TX)^{-1} + ww^T - ww^T\\
    & = \sigma^2(X^TX)^{-1}\\
\end{align*}

\subsubsection{Summary}
If the output variable $y \sim N(Xw, \sigma^2I)$ then $\E[w_{ML}]=w$ and $Var[w_{ML}]=\sigma^2(X^TX)^{-1}$

\begin{itemize}
    \item If $(X^TX)^{-1}$ has large values then the solution has a lot of variance, i.e. highly sensitive to data, i.e. over-fitting.
\end{itemize}

\subsubsection{Singular Value Decomposition}
For the $n \times (d+1)$ matrix $X$ we know that the SVD is given as, $X=USV^T$ where,
\begin{itemize}
    \item $U$ is $n \times d$ matrix and othronormal in columns, i.e. $U^TU=I$
    \item $V$ is $d \times d$ matrix and othronormal, i.e. $V^TV=VV^T=I$
    \item $S$ is a $d\times d$ non-negative diagonal matrix.
\end{itemize}

Then, if $X$ is full rank, i.e. $rank(X)=d$ or $s_{ii}\geq0, \forall i=1,2,\dots d$, we can write that, $$(X^TX)^{-1}=VS^{-2}V^T$$
Hence, if $S$ has very small entries, i.e. $X$ has very small singular values then the solution $w_{ML}$ will have a very high variance. Usually this would happen if columns of $X$ are highly correlated.

\subsection{Ridge Regression}

We will add a penalty term so that the solution does not have a very high variance. Adding a regularize term in the loss function as,

\begin{align*}
    \mathcal{L} &= \Vert Xw-y \Vert^2 + \lambda g(w)
\end{align*}

where,
\begin{itemize}
    \item $\lambda>0$ is the regularization parameter,
    \item $g(w) \geq 0$ is a penalty function that will help us achieve the desired properties of the solution.
\end{itemize} 

\subsubsection{L2 penalty}

Assuming,
\begin{align*}
    \mathcal{L} &= \Vert Xw-y \Vert^2 + \lambda \Vert w \Vert^2\\
    \implies w_{RR} &= (\lambda I + X^TX)^{-1}X^Ty\\
\end{align*}

\subsubsection{-- Comparison with Least Square solution}

\begin{align*}
    w_{RR} &= (\lambda I + X^TX)^{-1}X^Ty\\
    w_{RR} &= (\lambda I + X^TX)^{-1} (X^TX) w_{LS}\\
    w_{RR} &= (\lambda I + X^TX)^{-1} (X^TX) (X^TX)^{-1}X^Ty\\
    w_{RR} &= (\lambda (X^TX)^{-1} + I)^{-1} (X^TX)^{-1} (X^TX) w_{LS}\\
    w_{RR} &= (\lambda (X^TX)^{-1} + I)^{-1} w_{LS}\\
\end{align*}

Note that this implies, $\Vert w_{RR} \Vert_2 \leq \Vert w_{LS} \Vert_2\\$, i.e. ridge regression solution has L2 length smaller than the least square solution.

Moreover we have,

\begin{align*}
    w_{RR} &= (\lambda (X^TX)^{-1} + I)^{-1} w_{LS}\\
    w_{RR} &= (\lambda VS^{-2}V^T + I)^{-1} w_{LS}\\
    w_{RR} &= (\lambda VS^{-2}V^T + VV^T)^{-1} w_{LS}\\
    w_{RR} &= \left[V(\lambda S^{-2} + I)V^T\right]^{-1} w_{LS}\\
    w_{RR} &= V(\lambda S^{-2} + I)^{-1}V^T w_{LS}\\
    w_{RR} &= VMV^T w_{LS}\\
\end{align*}

where $M=(\lambda S^{-2} + I)^{-1}$, i.e., 
\begin{align*}
    M &= \begin{bmatrix}
        \frac{s_{11}^2}{s_{11}^2 + \lambda} & \dots & 0\\
        0 & \ddots & 0\\
        0 & \dots & \frac{s_{dd}^2}{s_{dd}^2 + \lambda}\\
    \end{bmatrix}
\end{align*}

Note,
\begin{itemize}
    \item $\lambda = 0 \implies M=I \implies w_{RR}=w_{LS}$
    \item $\lambda = \infty \implies w_{RR}=0$
\end{itemize}

\subsubsection{-- Is true solution expected ?}

Note that,
\begin{align*}
    \E[w_{RR}] &= \E[(\lambda I + X^TX)^{-1}X^Ty]\\
    &= (\lambda I + X^TX)^{-1}X^T \E[y]\\
    &= (\lambda I + X^TX)^{-1} X^TX w\\
\end{align*}

Hence, this method is \red{not an unbiased estimator of the true solution}.

\subsubsection{-- How much is the variation from the expected solution?}
Recall,
\begin{align*}
    \E[yy^T] &= \Sigma + \mu\mu^T\\
    \E[yy^T] &= \sigma^2I + Xww^TX^T\\
\end{align*}
Then,
\begin{align*}
    Var[w_{RR}] &= \E[(w-\E[w_{RR}])(w-\E[w_{RR}])^T]\\
    & = \E[w_{RR}w_{RR}^T]-\E[w_{RR}]\E[w_{RR}]^T\\
    & = \E[\left( (\lambda I + X^TX)^{-1} X^Ty \right) \left( (\lambda I + X^TX)^{-1} X^Ty \right)^T] \\
    & -\left( (\lambda I + X^TX)^{-1} X^TX w \right) \left( (\lambda I + X^TX)^{-1} X^TX w \right)^T]\\
    & = (\lambda I + X^TX)^{-1} X^T \E[yy^T] X (\lambda I + X^TX)^{-1} \\
    & - (\lambda I + X^TX)^{-1} X^TX ww^T X^T X (\lambda I + X^TX)^{-1}\\
    & = (\lambda I + X^TX)^{-1} X^T (\sigma^2I + Xww^TX^T) X (\lambda I + X^TX)^{-1} \\
    & - (\lambda I + X^TX)^{-1} X^TX ww^T X^T X (\lambda I + X^TX)^{-1}\\
    & = (\lambda I + X^TX)^{-1} X^T (\sigma^2I) X (\lambda I + X^TX)^{-1}\\
    & = \sigma^2 (\lambda I + X^TX)^{-1} X^T X (\lambda I + X^TX)^{-1}\\
    & = \sigma^2 (\lambda (X^TX)^{-1} + I)^{-1} (X^T X)^{-1} (\lambda (X^TX)^{-1} + I)^{-1}\\
    & = \sigma^2 Z (X^T X)^{-1} Z^T\\
\end{align*}

where, $Z = (\lambda (X^TX)^{-1} + I)^{-1}$, thus $\Vert ZZ^T \Vert_2 \leq 1$

clearly,
\begin{align*}
    \Vert Var[w_{RR}] \Vert_2 &= \Vert \sigma^2 Z (X^T X)^{-1} Z^T \Vert_2\\
    & = \Vert ZZ^T \Vert_2 \Vert Var[w_{LS}] \Vert_2\\
    & \leq \Vert Var[w_{LS}] \Vert_2\\
\end{align*}

\subsubsection{\red{How to select $\lambda$ ?}}
\subsubsection{\red{How to select $g(w)$ ?}}


\subsection{Least Square vs Ridge Regression ?}

Least Square:
\begin{itemize}
    \item Unbiased estimator
    \item Potentially high variance when singular values of X are small
\end{itemize}
\newpara
Ridge Regression:
\begin{itemize}
    \item Not an unbiased estimator
    \item Variance lower than Least Squares
\end{itemize}
\newpara
How to decide which one is better ?

\subsubsection{How well do they perform on new data ?}
We will try to measure the prediction error for a new data after learning parameters from the given dataset. Let $\hat{w}$ be the parameters learnt from the training data $X$ and $w$ be the true parameter such that $y \sim N(Xw, \sigma^2I)$. Let $x_0$ be a new data seen after the learning and $y_0$ the corresponding prediction. Then,

\begin{align*}
    \E[(y_0-x_0^T\hat{w})^2 \vert X, x_0] &= \E[y_0^2] - 2x_0^T\E[y_0\hat{w}] + x_0^T\E[\hat{w}\hat{w}^T]x_0\\
    &= (\sigma^2 + (x_0^Tw)^2) - 2x_0^T\E[y_0]\E[\hat{w}] + x_0^T (Var[\hat{w}] + \E[\hat{w}]\E[\hat{w}]^T) x_0\\
    &= \sigma^2 + (x_0^Tw)^2 - 2x_0^T (x_0^Tw) \E[\hat{w}] + x_0^T Var[\hat{w}]x_0 + x_0^T \E[\hat{w}]\E[\hat{w}]^T x_0\\
    &= \sigma^2 + (x_0^Tw)^2 - 2(x_0^Tw) x_0^T \E[\hat{w}] + x_0^T Var[\hat{w}]x_0 + (x_0^T \E[\hat{w}])^2\\
    &= \sigma^2 + \left\{ (x_0^Tw)^2 - 2(x_0^Tw) x_0^T \E[\hat{w}] + (x_0^T \E[\hat{w}])^2 \right\} + x_0^T Var[\hat{w}]x_0\\
    &= \sigma^2 + (x_0^T (w - \E[\hat{w}]))^2 + x_0^T Var[\hat{w}]x_0\\   
\end{align*}

Note that in above we have three types of errors,
\begin{itemize}
    \item \red{Measurement Noise:} $\sigma^2$ is the naturally occurring noise due to the variance which will always appear and cannot be reduced. Imagine that even when you know the exact distribution, you cannot predict the value correctly due to the variance.
    \item \red{Bias Error:} $(x_0^T (w - \E[\hat{w}]))^2$ is the error due to bias in parameter estimation and will be there if there is a difference in the estimated parameters and the true parameter. With our assumptions we have seen that the $w_{LS}=w$ so that there is no difference in the true and least square parameter value. So with some assumptions we can say that the least sqaure will have no bias. But ridge regression may have bias as $w_{RR} \neq w$ in general.
    \item \red{Variance Error:} $x_0^T Var[\hat{w}]x_0$ is the error due to variance in the parameter estimation. With our assumptions we have seen that the $w_{RR} \leq w_{LS}$ so this error will be less in ridge regression than in least squares.
\end{itemize}

\subsubsection{Bias vs Variance Trade-off}
In this particular case thus we have a bias-v-variance tradeoff where one method is good to reduce the error of one type only. The question is, can we generalize this idea ?

\subsubsection{-- Can error always be broken down in noise + bias + variance ? (Generalization)}
Assume, $y = f(x;w)+\epsilon$, s.t. $\E[\epsilon]=0$ and $Var(\epsilon)=\sigma^2$. Moreover, let $\hat{f}$ be a prediction of $f$ parameterized by $\hat{w}$, s.t. for a new data $x_0$ we can say that $y_0 \approx \hat{f}(x_0; \hat{w})$. Then,

\begin{align*}
    \E[(y_0-\hat{f}(x_0))^2] &= \E[y_0^2] - 2\E[y_0\hat{f}(x_0)] + \E[(\hat{f}(x_0))^2]\\
    &= \left\{ \sigma^2 + f^2(x_0;w) \right\} - 2\E[y_0]\E[\hat{f}(x_0)] + \left\{ Var(\hat{f}(x_0)) + \E[\hat{f}(x_0)]^2 \right\}\\
    &= \sigma^2 + \left\{ f_0^2 - 2f_0\E[\hat{f}_0] + \E[\hat{f}_0]^2 \right\} + Var(\hat{f}_0)\\
    &= \sigma^2 + (f_0 - E[\hat{f}_0])^2 + Var(\hat{f}_0)\\
\end{align*}

So we see that the error can always be broken down into these three error terms, but does this mean that there will always be a trade-off ?

\subsubsection{\red{-- Is the trade-off always expected?}} 

\subsection{\red{Lasso Regression}}
\subsection{\red{Maximum-a-posterior (MAP)}}
\subsection{\red{Greedy selection for under-determined system}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%% End of Linear Regression  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\hrfullline
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%% Begin Classification  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Classification}

\begin{itemize}
    \item In this case we have the data $(x_i,y_i)$ such that $y_i \in \mcaly$ and $\mcaly$ is discrete.
    \item For binary we usually use $\mcaly=\{0,1\}$ or $\mcaly=\{-1, +1\}$. For $k$ classes we use $\mcaly = \{1,2,\dots,k\}$.
    \item Although we have represented the classes numerically, the output is indeed classes and there is no order among the output possibilities, i.e. the mapping class to numeric is arbitrary.
\end{itemize}

\subsection{Nearest Neighbour}
\begin{itemize}
    \item Let $f$ denote the prediction function. Then for any new data $\xhat$, $f(\xhat) = y_i$ where $$i=\argmin_j d(\xhat, x_j)$$
    \item $d(.,.)$ is a distance function, usually taken to be the euclidean distance in case of real value data.
    \item 0 training error.
\end{itemize}
\subsubsection{k-NN}
\begin{itemize}
    \item Vote by majority of $k$ nearest neighbours.
    \item Smaller $k \implies$ 
        \begin{itemize}
            \item low bias (not much assumption about the model)
            \item high variance (depends on very low no. of points)
            \item smaller training error (0 for $k=1$)
        \end{itemize}
    \item Larger $k \implies$ high bias, low variance
        \begin{itemize}
            \item high bias (assumed that model depends on a large no. of points)
            \item low variance (more stable because of high no. of voting)
            \item larger training error (for ex. it is possible that a lot of points on the boundary are misclassified because of considering different points for the vote)
        \end{itemize}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%% Bayes Classifier %%%%%%%%%%%%%%%%%%

\subsection{Bayes Classifier (Optimum classifier in theory)}

\subsubsection{How to measure the error ? Can we get optimum classifier ?}
Optimum classifier is one which will minimize the prediction error. Let's define the prediction error for the random variable $(X,Y)$ as,
\begin{align*}
    err(f) &= P(f(X) \neq Y)\\
    &= \E_{X,Y}\left[\mathbbm{1} (f(X) \neq Y) \right]\\
    &= \E_{X}\left[ \E_{Y} \left[ \mathbbm{1} (f(X) \neq Y \vert X=x) \right] \right]\\
    &= \E_{X}\left[ \sum_{y \in \mcaly} P(Y=y \vert X=x) \cdot \left [\mathbbm{1} (f(x) \neq y) \right] \right]\\
\end{align*}

To minimize the above error, we can choose an optimal $f$, say $f_{optimal}$ such that,
\begin{align*}
    f_{optimal}(x) &= \argmax_{y \in \mcaly} P(Y = y \vert X = x)
\end{align*}
Which makes sense as the most natural classifier, one that gives the value of $y$ that has the maximum probability for a given $x$. Using Bayes rule,
\begin{align*}
    f_{optimal}(x) &= \argmax_{y \in \mcaly} P(X = x | Y = y) P(Y = y)
\end{align*}
where,
\begin{itemize}
    \item $P(Y=y)$ is called the \textit{class prior}
    \item $P(X=x|Y=y)$ is called the \textit{data likelihood}
    \item $f_{optimal}$ is thus called the Bayes classifier, which is the optimal classifier for this case.
    \item Note, above is only an optimal classifier if your objective is to minimize the overall prediction error. In alternative objectives, such as if you as predicting whether or not a person is sick and you don't want to classify someone negative when they are indeed positive (i.e. classification is false negative), you may want to \red{penalize once class more than the other}. In such a scenario, \red{Bayes classifier is not optimal for your objective}.
\end{itemize}


\subsubsection{-- Challenges}
\begin{itemize}
    \item We don't know the true probability $P(Y=y|X=x)$. We don't even know $P(Y=y)$ or $P(X=x)$.
    \item Above calculation only works under the assumption $(X,y) \sim^{iid} \mathcal{P}$
\end{itemize}

\subsubsection{-- Plug-in Classifier}
\begin{itemize}
    \item It is a classifier based on the above theory.
    \item Since we don't know the true distribution, we make some assumption about what that distribution is, such as, assume it to be Gaussian.
    \item We approximate the parameters of the distribution by using the data.
\end{itemize}

\subsubsection{-- Naive Bayes Classifier}
\begin{itemize}
    \item A family of Bayes classifier all featured by a naive assumption that conditional to a given value $Y=y$, all features for a given data $X=x=(x_1, \dots, x_d)$ are independent of each other. Hence,
        \begin{equation*}
            P(X=x|Y=y) =  \prod_{1 \leq j \leq d} P(x[j] = x_j | Y = y)
        \end{equation*}
    \item Above is a naive assumption because in some cases, the correlation is very visible. For example, in spam detection, correlation between features such as last word and current word is ignored.
    \item Easy to train, usually using MLE.
    \item Many variations exists, such as Gaussian, Multinomial, etc.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%% Log-of-odds %%%%%%%%%%%%%%%%%%
\subsection{Modelling log-of-odds in binary classification}
As we have seen already, the optimal $f$ is given as,
$$f_{optimal}(x) = \argmax_{y \in \mcaly} P(Y=y|X=x)$$
which can be written using the Bayes rule as,
$$f_{optimal}(x) = \argmax_{y \in \mcaly} P(X=x|Y=y)P(Y=y)$$

For an optimal binary classifier thus, $f_{optimal}(x) = 1$ if,
\begin{align*}
    P(X=x|Y=1)P(Y=1) &> P(X=x|Y=0)P(Y=0) \\
    \frac{P(X=x|Y=1)P(Y=1)}{P(X=x|Y=0)P(Y=0)} &> 1 \\
    \ln{\frac{P(X=x|Y=1)P(Y=1)}{P(X=x|Y=0)P(Y=0)}} &> 0 \\
\end{align*}

\begin{itemize}
    \item We can thus model the log-of-odds using some function and then use that for classification.
\end{itemize}

\subsubsection{Linear Classifier}

\begin{itemize}
    \item Will model the log-of-odds as a linear model such that $$f(x) = sign(x^Tw_1+w_0)$$
    \item Assumes that there is a linear sepreability in the domain for the two classes.
\end{itemize}

\subsubsection{-- Linear Discriminant Analysis (LDA)}
Consider a special case s.t. $P(x|y) = \mathcal{N}(x|\mu_y, \Sigma)$, $P(Y=1) = p1$, and $P(Y=0)=p0$, then,

\begin{align*}
    &\ln{\frac{P(X=x|Y=1)P(Y=1)}{P(X=x|Y=0)P(Y=0)}}\\
    &= \ln{\frac{\mathcal{N}(x|\mu_1,\Sigma)p1}{\mathcal{N}(x|\mu_0,\Sigma)p0}} \\
    &= \ln{\frac{\left[\frac{\exp{\left(-\frac{1}{2}(x-\mu_1)^T\Sigma^{-1}(x-\mu_1)\right)}}{\sqrt{(2\pi)^d|\Sigma|}}p1\right]}{\left[\frac{\exp{\left(-\frac{1}{2}(x-\mu_0)^T\Sigma^{-1}(x-\mu_0)\right)}}{\sqrt{(2\pi)^d|\Sigma|}}p0\right]}} \\
    &= \ln{\frac{p1}{p0}}-\frac{1}{2}(x-\mu_1)^T\Sigma^{-1}(x-\mu_1) + \frac{1}{2}(x-\mu_1)^T\Sigma^{-1}(x-\mu_1)\\
    &= \red{\ln{\frac{p1}{p0}}-\frac{1}{2}(\mu_1+\mu_0)^T\Sigma^{-1}(\mu_1-\mu_0)} + x^T\blue{\Sigma^{-1}(\mu_1-\mu_0)}\\
    &= \red{w_0} + x^T\blue{w_1}\\
\end{align*}

\begin{itemize}
    \item The Bayes is one instance of a linear classifier.
\end{itemize}

\subsubsection{-- Quadratic Discriminant Analysis (QDA)}

\begin{itemize}
    \item Assume, $f(x) = sign(x^Tw_2x + x^Tw_1 + w_0)$. Note that, the model is still linear in $(w_0, w_1, w_2)$.
    \item We can show that a Gaussian model for log of odds with different co-variance matrices $(P(X=x|Y=y_i) = \mathcal{N}(x|\mu_i,\Sigma_i))$ is an instance of QDA.
\end{itemize}

\subsubsection{-- Least squares for Linear classification}

\begin{itemize}
    \item A more general approach to not constraint the parameters in linear classifier to have any meaning and allowing them to take any value by modelling it as a least square problem.
    \item Not recommended, too sensitive to the outliers.
    
\end{itemize}

\subsubsection{-- Perceptron Algorithm}
\begin{itemize}
    \item Model as, $y = f(x;w) = sign(x^Tw)$
    \item Since there is no analytical solution, use an iterative algorithm to find the solution to the optimal parameter value.
    \item We model the loss as, $$\mathcal{L} = -\sum_{1 \leq i \leq n}{ y_i\cdot sign(x_i^Tw)}\cdot \mathbbm{1}_{y_i \neq sign(x_i^Tw)}$$
    \item Thus, $$\nabla_w{\mathcal{L}} = -\sum_{\{i | {y_i \neq sign(x_i^Tw)}\}}{ y_i\cdot x_i}$$
    \item Thus the step of a stochastic gradient descent will be, $$w^{(t+1)} = w^t - \eta\nabla_w{\mathcal{L}}$$
    \item \red{Note that we count the loss against the mismatch as 1 or nothing, there is no partial loss}. This basically because there is no degree to which a point belongs to a particular class, the model strictly gives $\pm1$.
    \item Note that if there is one optimal solution, there are infinitely many optimal solutions and the algorithm does not favour one or the other.
    \item It is possible that the algorithm does not converge at all.
\end{itemize}

\subsubsection{-- Logistic Regression}
Recall that, we saw below pros and cons for the last two models,
\begin{itemize}
    \item Least Square
        \begin{itemize}
            \item Pro: Gives a probability (degree) to which an input belongs to a particular class
            \item Con: Too sensitive to the outliers (high variance)
        \end{itemize}
    \item Perceptron Algorithm
        \begin{itemize}
            \item Pro: Gives (an optimal solution) if there is a linear separability
            \item Con: Assumes linear separability, convergence issues
        \end{itemize}
\end{itemize}

We will now aim for a model which has above pros and overcomes the cons. Specifically, we are looking for,
\begin{itemize}
    \item Gives a probability to which the input belongs to a class.
    \item Is not too sensitive to the outliers, or there is a way to add penalty to decrease the variance.
    \item Does not assume linear separability.
\end{itemize}

Let's model the log-of-odds as a linear function. Also, not using the Bayes rule yet, thus,
\begin{align*}
    \ln{\frac{P(Y=1|X=x)}{P(Y=-1|X=x)}} &= x^Tw_1+w_0\\
    \ln{\frac{P(Y=1|X=x)}{1-P(Y=1|X=x)}} &= x^Tw_1+w_0\\
    P(Y=1|X=x) &= \frac{\exp{(x^Tw_1+w_0)}}{1+\exp{(x^Tw_1+w_0)}}\\
    P(Y=1|X=x) &= \sigma(x^Tw_1+w_0)\\
\end{align*}

where $\sigma(x) = \frac{e^x}{1+e^x}$ is the sigmoid function and $(x^Tw_1+w_0)$ is called the link function for the log odds.
\newpara
Let $\xhat = \begin{bmatrix}1\\x\end{bmatrix}$, $w = \begin{bmatrix}w_0\\w_1\end{bmatrix}$ and $\sigma_i(w) = \sigma(\xhat_i^Tw)$, then, the joint likelihood of labelling the entire data-set becomes,
\begin{align*}
    p(y_1,\dots,y_n | x_1, \dots, x_n, w) &= \prod_{i=1}^n p(y_i|x_i,w)\\
    &= \prod_{i=1}^n \sigma_i(w)^{\mathbbm{1}(y_i = +1)} (1-\sigma_i(w))^{\mathbbm{1}(y_i = -1)} \\
    &= \prod_{i=1}^n \sigma_i(y_i\cdot w) \\
\end{align*}
The corresponding MLE solution will be,
\begin{align*}
    w_{ML} & = \argmax_w \prod_{i=1}^n \sigma_i(y_i\cdot w) \\
    w_{ML} & = \argmax_w \sum_{i=1}^n{\ln{\sigma_i(y_i\cdot w)} }\\
    w_{ML} & = \argmax_w \mathcal{L}(w) 
\end{align*}

Since we cannot solve it analytically, we will use an algorithm like gradient ascent (for maximization).
\begin{align*}
    \nabla_w \mathcal{L} &= \sum_{i=1}^n (1-\sigma_i(y_i \cdot w))y_ix_i\\
    w^{t+1} &= w^t + \eta \nabla_w\mathcal{L}
\end{align*}
Note that the gradient in Perceptron algorithm vs logistic regression differ by a multiplicative factor of $(1-\sigma_i(y_i\cdot w))$ which represents the probability by which it was wrongly assigned to a particular class.

\newpara
Logistic regression suffers from over-fitting if the data is linearly separable. This can be seen as if $||w_{ML}|| \to \infty$ then $(1-\sigma_i(y_i\cdot w_{ML})) \to 0$. So in practice, a regularizer is used such that,
\begin{align*}
    \mathcal{L}(w) &= \sum_{i=1}^n{\ln{\sigma_i(y_i\cdot w)} } - \lambda w^Tw\\
\end{align*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%% End of Classification  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\hrfullline
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%% Intro to Unsupervised Learning  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Unsupervised Learning}
\begin{tabularx}{\textwidth} { 
  | >{\raggedright\arraybackslash}X 
  | >{\raggedright\arraybackslash}X 
  | >{\raggedright\arraybackslash}X | }
\hline\\
            & Supervised & Unsupervised \\
        \hline
            Input & $(x_i, y_i)$ pairs & Only $x_i$\\
        \hline
            \multirow{2}{*}{Probabilistic View} & Learning the joint distribution $p(x,y)$ & Learning the underlying distribution of $p(x)$\\
            \cline{2-3}
            & $p(x,y) = p(x|y)p(y) = p(y|x)p(x)$, we model either $p(x|y)p(y)$ (example in Bayes Classifier) or we model $p(y|x)$ (example in logistic regression). We don't generally model $p(x)$ as that is assumed to be given to us as input. & We model $p(x)$ always, since there is no $y$, other distributions are not involved. \\
        \hline
\end{tabularx}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%% End of Unsupervised Learning  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\hrfullline
\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%% Begin Clustering  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Clustering}

\begin{itemize}
    \item Clustering is similar to labelling the data (classification) but we don't know the ground truth, i.e. training data is not labelled.
    \item We want to cluster similar points and divide different points. But, what do we mean by "similarity"? Some ideas for what this might mean:
        \begin{itemize}
            \item Distance between the points in one cluster is "small". Some questions:
                \begin{itemize}
                    \item What is the distance metric?
                    \item What quantifies as small?
                \end{itemize}
        \end{itemize}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%% K-means %%%%%%%%%%%%%%%%%%

\subsection{K-means}

\begin{itemize}
    \item \textbf{Input} $x_1, \dots, x_n \in \mathbb{R}^d$
    \item \textbf{Output} $c=(c_1, \dots, c_n)$, s.t. $c_i \in \{1, \dots, K\}$ and $\mu=(\mu_1, \dots, \mu_K) \in \mathbb{R}^d$
    \item $c_i=k \implies$ the point $x_i$ is clustered in the cluster indexed $k$
    \item $\mu_k$ denotes the centroid of the cluster with index $k$
    \item \textbf{Objective Function} $$\argmin_{\mu, c} \sum_{i=1}^n \sum_{k=1}^K \mathbbm{1}_{\{c_i=k\}} \Vert x_i-\mu_k \Vert_2$$
    \item The objective function is \red{non-convex} and not-smooth(as $c$ is not even a continuous variable?)
\end{itemize}

\subsubsection{Coordinate Descent}
\begin{itemize}
    \item Algorithm
        \begin{itemize}
            \item Fix $K$ (\red{How?} check below)
            \item Repeat the following until $c$ and $\mu$ don't change or you reach some max iteration
            \begin{itemize}
                \item Find best $\mu$ assuming the given clustering $c$ for the points
                    $$\mu_k = \frac{\sum_{i=1}^n \mathbbm{1}_{\{c_i=k\}} x_i}{\sum_{i=1}^n \mathbbm{1}_{\{c_i=k\}} 1}$$
                \item Find best $c$ given the centroid $\mu$ for the clusters
                    $$c_i = \argmin_{k} \Vert x_i-\mu_k \Vert_2$$
            \end{itemize}
        \end{itemize}
    \item Convergence
        \begin{itemize}
            \item Loss function is monotonically (but not strictly) decreasing implying convergence of some sort
            \item Not optimal solution
            \item Initialization matters
        \end{itemize}
\end{itemize}

\subsubsection{Choosing $K$}
\begin{itemize}
    \item Note, $K=n \implies \mu_k = x_k$ solves the problem with the minimum loss but does not help us at all as each point is its own cluster
    \item Prior knowledge based on subject matter such as splitting a task against $K$ people
    \item Methods that select $K$ by relative decrease in the loss, such as \red{Elbow} method and \red{Silhouette} method
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%% Weighted K-means %%%%%%%%%%%%%%%%%%

\subsection{Weighted K-means}

\begin{itemize}
    \item \textbf{Input} $x_1, \dots, x_n \in \mathbb{R}^d$
    \item \textbf{Output} $\phi=(\phi_1, \dots, \phi_n)$, s.t. $\phi_i \in \mathbb{R}^K_+$, $\sum_{k=1}^K\phi_i(k)=1$ and $\mu=(\mu_1, \dots, \mu_K)$, $\mu_k \in \mathbb{R}^d$
    \item $\phi_i(k)$ denotes the weight with which $x_i$ belongs to the cluster $k$
    \item $\mu_k$ denotes the centroid of the cluster with index $k$
    \item \textbf{Objective Function} $$\argmin_{\phi, \mu} \left( \sum_{i=1}^n \sum_{k=1}^K \phi_i(k) \frac{\Vert x_i-\mu_k \Vert_2}{\beta} - \sum_{i=1}^n\mathcal{H}(\phi_i) \right)$$
    \item $\mathcal{H}(\phi_i) = -\sum_{k=1}^K \phi_i(k)\ln{\phi_i(k)}$ is the entropy of the function $\phi_i$.
    \item The objective function is \red{non-convex}\blue{?}
\end{itemize}

\subsubsection{Understanding the loss function}
\begin{itemize}
    \item \textbf{Why the entropy term ?}
        \begin{itemize}
            \item If we only have the distance term to minimize, the $\phi_i$ vectors learnt will be the one-hot encoding vector, as the loss is minimum when all of the weight of a point is assigned to the cluster which has least distance from the point.
            \item What is wrong with having one-hot vectors? This results in hard-clustering, the aim of this algorithm was to have a soft-clustering
            \item Since loss has -ve in front of entropy, we have to maximize the entropy value.
            \item Entropy of a vector $\phi_i$ will be maximum when $\phi_i(k)=\frac{1}{k}$. Hence this term will force the objective towards a more distributed clustering
            \item \blue{Random thought - this seems like the discriminator in the GANs, which regularizes the objective towards a prior distribution}
        \end{itemize}
    \item \textbf{Why $\beta$}
        \begin{itemize}
            \item \red{I have no good answer}, it seems similar to dividing by a standard deviation but assumed a constant across all clusters so not sure. It does somehow represent the width of the cluster to take into account.
        \end{itemize}
\end{itemize}

\subsubsection{Coordinate Descent}
\begin{itemize}
    \item Algorithm
        \begin{itemize}
            \item Fix $K$ (\red{How?} check in K-means)
            \item Repeat the following until $\phi$ and $\mu$ don't change or you reach some max iteration
            \begin{itemize}
                \item Find best $\phi$ given the centroid $\mu$ for the clusters
                    $$\phi_i(k) = \frac{ e^{-\frac{\Vert x_i-\mu_k \Vert_2}{\beta} } } { \sum_{j=1}^K e^{-\frac{\Vert x_i-\mu_j \Vert_2}{\beta}} }$$
                \item Find best $\mu$ assuming the given weights $\phi$
                    $$\mu_k = \frac{\sum_{i=1}^n \phi_i(k) x_i}{\sum_{i=1}^n \phi_i(k)}$$
            \end{itemize}
        \end{itemize}
    \item Convergence
        \begin{itemize}
            \item Same points as K-means \blue{?}
        \end{itemize}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%% Mixture Model %%%%%%%%%%%%%%%%%%

\subsection{Mixture Model}

\begin{itemize}
    \item \blue{TODO}
\end{itemize}

\subsubsection{EM Algorithm}
\begin{itemize}
    \item \blue{TODO}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%% End of Classification  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\hrfullline
\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%% Begin Dimensionality Reduction %%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Dimensionality Reduction}

%%%%%%%%%%%%%%%%%%%%%%%%%%%% PCA %%%%%%%%%%%%%%%%%%

\subsection{Principal Component Analysis (PCA)}

\subsubsection{Starting from Basics : Reduction to one dimensional vector}
\begin{itemize}
    \item \textbf{Input} $\{x_1, \dots, x_n\} \in \mathbb{R}^d$
    \item \textbf{Output} $q \in \mathbb{R}^d$, s.t. $\Vert q \Vert_2=1$ and $q$ is a unit vector in a direction along which the projection of $x_i$ are optimal in some sense? Q: What does optimal mean here? A: Sum of Eucledian distance between the points and there respective projection is minimum.
    \item \textbf{Objective} 
        $$\argmin_{q} \sum_{i=1}^n \Vert x_i - (q^Tx_i)q \Vert_2^2  $$
    \item \textbf{Solution} 
        \begin{align*}
            & \argmin_{q} \sum_{i=1}^n \Vert x_i - (q^Tx_i)q \Vert_2^2 \\
            &= \argmin_{q} \sum_{i=1}^n \Vert x_i - qq^Tx_i \Vert_2^2 \\
            &= \argmin_{q} \sum_{i=1}^n (x_i^T - x_i^Tqq^T)(x_i - qq^Tx_i) \\
            &= \argmin_{q} \sum_{i=1}^n \left( \Vert x_i \Vert_2^2 - x_i^Tqq^Tx_i - x_i^Tqq^Tx_i + x_i^Tqq^Tqq^Tx_i \right)\\
            &= \argmin_{q} \sum_{i=1}^n \left( \Vert x_i \Vert_2^2 - q^T(x_ix_i^T)q \right)
            = \argmax_{q} q^T( \sum_{i=1}^n x_ix_i^T)q\\
            &= \argmax_{q} q^T( XX^T )q = \argmax_{q} \Vert X^Tq \Vert_2^2
        \end{align*}
        Recall that the spectral norm of a matrix $X^T$ is given by
        $$\Vert X^T\Vert_2 = \sup_{y \neq 0} \frac{\Vert X^Ty \Vert_2}{\Vert y \Vert_2} = \sup_{\Vert y \Vert_2 = 1} \Vert X^Ty \Vert_2 = \lambda_{\text{max}}(XX^T)$$
        Hence, $q$ is the unit vector along the largest eigenvector of the matrix $XX^T$ where $X = [x_1, \dots, x_n]$
\end{itemize}

\subsubsection{Reduction to K dimensional vector}
\begin{itemize}
    \item \textbf{Input} $\{x_1, \dots, x_n\} \in \mathbb{R}^d$
    \item \textbf{Output} $Q=[q_1, \dots, q_K]$, s.t. $q_k \in \mathbb{R}^d$ and thus $Q \in \mathbb{R}^{d \times K}$ and $Q^TQ=I$
    \item \textbf{Objective} 
        $$\argmin_{Q} \sum_{i=1}^n \Vert x_i - \sum_{k=1}^K(q_k^Tx_i)q_k \Vert_2^2  $$
    \item \textbf{Solution} 
        \begin{align*}
            & \argmin_{Q} \sum_{i=1}^n \Vert x_i - \sum_{k=1}^K(q_k^Tx_i)q_k \Vert_2^2\\
            &= \argmin_{Q} \sum_{i=1}^n \Vert x_i - \sum_{k=1}^K q_kq_k^Tx_i \Vert_2^2 \\
            &= \argmin_{Q} \sum_{i=1}^n (x_i^T - \sum_{k=1}^K x_i^Tq_kq_k^T)(x_i - \sum_{k=1}^K q_kq_k^Tx_i) \\
            &= \argmin_{Q} \sum_{i=1}^n \left( \Vert x_i \Vert_2^2 - \sum_{k=1}^K x_i^Tq_kq_k^Tx_i - \sum_{k=1}^K x_i^Tq_kq_k^Tx_i + \sum_{k=1}^K\sum_{j=1}^K x_i^Tq_kq_k^Tq_jq_j^Tx_i \right)\\
            &= \argmin_{Q} \sum_{i=1}^n \left( \Vert x_i \Vert_2^2 - \sum_{k=1}^K x_i^Tq_kq_k^Tx_i - \sum_{k=1}^K x_i^Tq_kq_k^Tx_i + \sum_{k=1}^K x_i^Tq_kq_k^Tx_i \right)\\
            &= \argmin_{Q} \sum_{i=1}^n \left( \Vert x_i \Vert_2^2 - \sum_{k=1}^K x_i^Tq_kq_k^Tx_i \right)
            = \argmax_{Q} \sum_{i=1}^n q_k\sum_{k=1}^K x_i^Tx_iq_k^T \\
            &= \argmax_{Q} \sum_{k=1}^K q_k^T( XX^T )q_k
        \end{align*}
        Clearly $q_1$ is a unit vector in the direction of eigenvector corresponding to largest eigenvalue. Since, $q_1^Tq_2=0 \implies q_2\neq q_1 \implies q_2$ is a unit vector in the direction of eigenvector corresponding to second-largest eigenvalue.  And so on ...
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%% End of Dimensionality Reduction %%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\hrfullline
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%% Begin References  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{References}
\begin{itemize}
    \item Prof John Paisely Lectures for Machine Learning - Spring 2020 Columbia University
\end{itemize}


\end{document}
