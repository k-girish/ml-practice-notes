\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb,bbm}
\usepackage{tikz}
\usepackage{multirow, tabularx}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\E}{\mathop{\mathbb{E}}}

\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\newpara}{\leavevmode\newline}
\newcommand{\hrfullline}{\noindent\makebox[\linewidth]{\rule{\paperwidth}{2pt}}}
\newcommand{\mcaly}{\mathcal{Y}}
\newcommand{\xhat}{\hat{x}}

\title{Autoencoder}
\author{Girish Kumar }
\date{Mar 2021}

\begin{document}
\maketitle

We are optimizing under the Frobenius norm. Consider the simple case that the model is given by $f(X) = XW$, where $X \in \mathbb{R}^{n \times d}$ and $W \in \mathbb{R}^{d \times d}$. Then,
\begin{align*}
    L(X) &= ||XW-X||_F^2\\
        &= tr((XW-X)^T(XW-X))\\
        &= tr(W^TX^TXW - W^TX^TX - X^TXW + X^TX)\\
    \frac{dL}{dW} &= 2X^TX(W -I)
\end{align*}
In case of pseudo weighted Forbenius norm,
\begin{align*}
    L(X) &= ||XWD-XD||_F^2\\
        &= tr((XWD-XD)^T(XWD-XD))\\
        &= tr(D^TW^TX^TXWD - D^TW^TX^TXD - D^TX^TXWD + D^TX^TXD)\\
    \frac{dL}{dW} &= 
\end{align*}


\end{document}
