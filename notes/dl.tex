%! Author = girish
%! Date = 3/26/21

% Preamble
\documentclass[11pt]{article}

% Packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{color}
\usepackage{hyperref}

\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\xhat}{\hat{x}}
\newcommand{\yhat}{\hat{y}}

% Document
\begin{document}

\tableofcontents
\newpage
    \section{Best practices}
    \begin{itemize}
        \item Many paper prefer Leaky-Relu over Relu (usually with $\alpha$ = 0.1) so that \red{gradient does not vanish?}
        \item Adam optimization (0.9, 0.99) is almost always preferred (unless you want DP then one has to use stochastic gradient)
    \end{itemize}


    \section{Normalization}
    \red{Expand below topics}
    \begin{itemize}
        \item Batch-normalization
        \item Layer-normalization
    \end{itemize}


    \section{Convolution}
    \red{Conv 2D - upscale}


    \section{Gradient Clipping}
    \red{Add more here}
    \begin{itemize}
        \item Hard Constraints:
        \item Clip magnitude of each individual weight
        \item L2 norm clipping
        \item Weight normalization
        \item Soft Constraints:
        \item L1 weight Decay
        \item L2 weight Decay
    \end{itemize}


    \section{Generative Adversarial Networks (GAN)}

    \begin{itemize}
        \item Generative because we are modelling the probability distribution of $p(x)$ or if we have labels $p(x,y)$. This probability distribution can be used to \red{sample synthetic data}.
        \item Contains two sub-networks - Generator and Discriminator - they both compete in a game trying to defeat each other
        \item \red{Generator's} goal is to learn how to produce \red{fake data} that appear real.
        \item \red{Discriminator's} goal is to identify that images generated by the \red{generator are fake}. Thus discriminator is simply a \red{classifier}.
        \item Training a GAN
        \begin{itemize}
            \item \red{Alternate training} between generator and discriminator, i.e. don't update the weights of discriminator when updating generator and vice versa
            \item N-times training of the discriminator for each generator training - N = 5 for DNA paper
            \item During discriminator training, gradient is calculated only on discriminator loss
            \item During generator training, gradient is calculated only on generator loss
            \item After some time,
            \begin{itemize}
                \item \red{generator becomes more efficient} in producing fake images
                \item \red{discriminator becomes less accurate} in distinguishing
                \item generator is not getting an efficient training since the discriminator feedback is less efficient
                \item the training may \red{not convergence}
            \end{itemize}
        \end{itemize}
        \item Loss function
        \begin{itemize}
            \item In general we need two loss terms, one for each type of sub-network. Some commonly used losses-
            \begin{itemize}
                \item \red{Minimax Loss}
                $$\min_G \max_D \left( E_{x}{[\log {D(x)}]} + E_{z}{[\log{1-D(G(z))}]} \right)$$
                Sometimes gets stuck in early stages, prefer Wasserstein Loss
                \item \red{Modified Minimax Loss}
                \begin{itemize}
                    \item Generator: maximize $E_{z}{[\log{D(G(z))}]}$
                    \item Discriminator: same
                    \item Overcomes getting stuck in early stages
                \end{itemize}
                \item \red{Wasserstein Loss}
                $$\min_G \max_D \left( D(x)-D(G(z)) \right)$$
                Output of $D(x) \in \mathbb{R}$ is not assumed to be a probability, the discriminator aims to produce $D(x)$ as large as possible for real data. Hence the discriminator is more of a critic. \blue{Read more from paper}

            \end{itemize}
        \end{itemize}
        \item Common GAN problems
        \begin{itemize}
            \item \red{Vanishing gradient} - Discrimnator too good $\implies$ gradient vanishes, generator training is impacted. Solution:
            \begin{itemize}
                \item Wasserstein Loss
                \item Modified minimax loss
            \end{itemize}
            \item \red{Mode Collapse} - If generator is only focusing on a small subset of the original data and discriminator is stuck in some local minima. Solution:
            \begin{itemize}
                \item Wasserstein Loss
                \item Unrolled GANs \blue{?}
            \end{itemize}
            \item \red{Failed to converge} - Usually seen when discriminator is too accurate. Solution:
            \begin{itemize}
                \item Adding noise to discriminator input
                \item Regularize discriminator weights
                \item \blue{read more about above}
            \end{itemize}
        \end{itemize}
    \end{itemize}

    \subsection{Measuring performance of GANs}

    \subsubsection{By Visualization}
    \begin{itemize}
        \item Latent Interpolation - Smooth transition in the output based when linear interpolation between values in latent space.
        \item Latent Complementation/Reflection - Assuming that the latent space is symmetric about origin, see if the reflection of a point in the latent space produces a compliment behaviour in the data space as well.
        \item
    \end{itemize}


    \section{Wasserstein Autoecnoder (WAE)}
    \begin{itemize}
        \item \blue{?}
    \end{itemize}

    \section{Autoencoder}
    \begin{itemize}
        \item Typically a neural network that is trained to approximately reconstruct the input in the hope of learning critical features about the data
        \item Typical structure is Input -> Encoder -> Latent Space -> Decoder -> reconstruction, where the encoder network is say e, decoder network is say d, and $\xhat = d(z), z = e(x), \xhat=d(e(x))$.
        \item Some types of autoencoders:
            \begin{itemize}
                \item \textbf{Undercomplete Autoencoder}
                    \begin{itemize}
                        \item Dimension of z is much smaller than x
                        \item Usually used for feature extraction / dimensionality reduction
                        \item \blue{Note:} Too much power of encoder/decoder may result in the network not learing any critical information of the data
                    \end{itemize}
                \item \textbf{Sparse Autoencoder}
                    \begin{itemize}
                        \item Forcing the latent variable to follow a sparse representation by adding a penalty term for it in the loss
                        \item This can also be thought of as introducing a prior in the latent variable space for generative network trained for maximum likelihood (see deeplearinng book for explanation)
                    \end{itemize}
                \item \textbf{Denoising Autoencoder}
                    \begin{itemize}
                        \item \red{?}
                    \end{itemize}
                \item \textbf{Contractive Autoencoder}
                    \begin{itemize}
                        \item Use the derivative of encodings as a regularization, ex $\lambda \cdot \sum_i || \nabla_x z_i ||_F$
                        \item Forces smoothness in the encoded space
                    \end{itemize}
            \end{itemize}
    \end{itemize}


    \section{Variational Autoencoder}
    \begin{itemize}
        \item Learns a probability distribution for the latent representation vector $z$ that \red{maximizes the likelihood of given data $x$}, i.e. objective is to maximize
        $$p(x) = \int p(x|z)p(z)dz$$
        \item It is a feed forward neural network with a \red{bottleneck layer} in between. This layer acts as the latent space
        \item Neural network input and output is the same value x and can broken down in to \red{Encoder and Decoder} - hence the name autoencoder
        \item Assume parameters $\theta$ and $\phi$ that parameterize the probability functions $q_\phi(z|x)$ (for encoder) and $p_{\theta}(x|z)$ (for decoder)
        \item We want to maximize the objective function can be shown to be for a particular point $x$
        \begin{align*}
            \mathcal{L}(\theta, \phi) &= E_{q_\phi(z|x)}{[\log{p_\theta(x|z)}]}  - D_{KL}(q_\phi(z|x)||p_\theta(z)) \\
            &= \frac{1}{L}\sum_{l=1}^L \left( \log {p_\theta(x|z^{(l)})} \right) - D_{KL}(q_\phi(z|x)||p_\theta(z))\\
            \text{where } z^{(l)} \sim q_\phi(z|x)
        \end{align*}
        \item \red{First term is inversely proportional to the reconstruction loss}, if we assume $p_\theta(x|z) = \mathcal{N}(x;G_\theta(z), \eta I)$, then
        $$\log{p_\theta(x|z)}=\frac{-1}{2\eta} \Vert x - G_\theta(z) \Vert^2 + \text{ const}$$
        \item \red{Second term is acting as regularizer}, without this term, $q_\phi(z|x)$ would just be point mass distribution, with mass concentrated on $x$ from the input data. KL divergence forces $q_\phi(z|x)$ to follow a pre-defined distribution $p_theta(z)$ (usually Gaussian).
        \item \blue{Re-parameterization trick}
        \item We can use the latent space to further \red{sample synthetic data}

    \end{itemize}

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    %%%%%%%%%%%%%%%%%%%%% REFERENCES %%%%%%%%%%%%%%%%%%%%%%%%%%%%
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


    \section{References}
    \begin{itemize}
        \item \href{https://developers.google.com/machine-learning/gan}{Google Developer course on GANs}
    \end{itemize}

\end{document}